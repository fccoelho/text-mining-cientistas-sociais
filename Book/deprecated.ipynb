{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10d4b0d-a54b-433c-8801-d5b800b5c828",
   "metadata": {},
   "source": [
    "# Deprecated cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b8cf9-a18e-4716-b757-870e9022dd67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Treinando um Identificador de Entidades a partir do DHBB\n",
    "Identificadores de entidades são algoritmos treinados em corpora manualmente anotados. Como necessitamos reconhecer outros tipos de entidades, precisamos retreinar o modeld usando o próprio DHBB. Para este fim utilizaremos os dicionários já disponíveis no DHBB,  juntamente com o índice construído no capítulo 2 para recuperar o contexto de cada entrada dos dicionários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c70bd751-17d7-4bd0-9c6a-f95d24f0e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-26 11:54:35.372031: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.10/dist-packages/cv2/../../lib64:\n",
      "2022-11-26 11:54:35.372088: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.10/dist-packages/cv2/../../lib64:\n",
      "2022-11-26 11:54:35.372095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-26 11:54:36.506108: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.10/dist-packages/cv2/../../lib64:\n",
      "2022-11-26 11:54:36.506135: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pt-core-news-lg==3.4.0\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download pt_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83e34d1f-3b10-49d7-bf89-379e058b6e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh import index \n",
    "import os\n",
    "from whoosh.qparser import QueryParser\n",
    "from whoosh import qparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb42145-bf0f-44fe-94e4-1e226558b2d3",
   "metadata": {},
   "source": [
    "O primeiro passo é abrirmos o nosso indice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa597947-21cd-4ede-9e0e-6b7e2f37d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('indexdir'):\n",
    "    indice = index.open_dir('indexdir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ad9742b-bbac-49d7-be32-f97b471539c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26896"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indice.doc_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79b62753-fd57-4a09-a44c-7c2615630f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca(consulta):\n",
    "    qp = QueryParser(\"corpo\", indice.schema)\n",
    "    qp.add_plugin(qparser.EveryPlugin())\n",
    "    query = qp.parse(consulta)\n",
    "    \n",
    "    with indice.searcher() as searcher:\n",
    "        results = [(dict(hit),hit.highlights(\"corpo\")) for hit in searcher.search(query, limit=10)]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d524c809-3b9d-47f9-a957-d1d33fdc8d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados: 10\n",
      "NOGUEIRA FILHO, Paulo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Paulo Nogueira <b class=\"match term0\">Filho</b>» nasceu na cidade <b class=\"match term1\">de</b> São Paulo no dia 16 <b class=\"match term1\">de</b>\n",
       "novembro <b class=\"match term1\">de</b> 1898, <b class=\"match term2\">filho</b> <b class=\"match term1\">de</b> Paulo <b class=\"match term1\">de</b> Almeida Nogueira e <b class=\"match term1\">de</b> Ester\n",
       "Nogueira. Seu avô...<b class=\"match term0\">Filho</b> publicou artigo no Diário <b class=\"match term1\">de</b>\n",
       "São Paulo <b class=\"match term1\">de</b> 19 <b class=\"match term1\">de</b> abril <b class=\"match term1\">de</b> 1928 intitulado “O pensamento...<b class=\"match term0\">Filho</b> morreu em São Paulo no dia 29 <b class=\"match term1\">de</b> outubro <b class=\"match term1\">de</b> 1969.\n",
       "\n",
       "Além <b class=\"match term1\">de</b> discursos e conferências"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOGUEIRA FILHO, Paulo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Paulo Nogueira <b class=\"match term0\">Filho</b>» nasceu na cidade <b class=\"match term1\">de</b> São Paulo no dia 16 <b class=\"match term1\">de</b>\n",
       "novembro <b class=\"match term1\">de</b> 1898, <b class=\"match term2\">filho</b> <b class=\"match term1\">de</b> Paulo <b class=\"match term1\">de</b> Almeida Nogueira e <b class=\"match term1\">de</b> Ester\n",
       "Nogueira. Seu avô...<b class=\"match term0\">Filho</b> publicou artigo no Diário <b class=\"match term1\">de</b>\n",
       "São Paulo <b class=\"match term1\">de</b> 19 <b class=\"match term1\">de</b> abril <b class=\"match term1\">de</b> 1928 intitulado “O pensamento...<b class=\"match term0\">Filho</b> morreu em São Paulo no dia 29 <b class=\"match term1\">de</b> outubro <b class=\"match term1\">de</b> 1969.\n",
       "\n",
       "Além <b class=\"match term1\">de</b> discursos e conferências"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOGUEIRA FILHO, Paulo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Paulo Nogueira <b class=\"match term0\">Filho</b>» nasceu na cidade <b class=\"match term1\">de</b> São Paulo no dia 16 <b class=\"match term1\">de</b>\n",
       "novembro <b class=\"match term1\">de</b> 1898, <b class=\"match term2\">filho</b> <b class=\"match term1\">de</b> Paulo <b class=\"match term1\">de</b> Almeida Nogueira e <b class=\"match term1\">de</b> Ester\n",
       "Nogueira. Seu avô...<b class=\"match term0\">Filho</b> publicou artigo no Diário <b class=\"match term1\">de</b>\n",
       "São Paulo <b class=\"match term1\">de</b> 19 <b class=\"match term1\">de</b> abril <b class=\"match term1\">de</b> 1928 intitulado “O pensamento...<b class=\"match term0\">Filho</b> morreu em São Paulo no dia 29 <b class=\"match term1\">de</b> outubro <b class=\"match term1\">de</b> 1969.\n",
       "\n",
       "Além <b class=\"match term1\">de</b> discursos e conferências"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOGUEIRA FILHO, Paulo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Paulo Nogueira <b class=\"match term0\">Filho</b>» nasceu na cidade <b class=\"match term1\">de</b> São Paulo no dia 16 <b class=\"match term1\">de</b>\n",
       "novembro <b class=\"match term1\">de</b> 1898, <b class=\"match term2\">filho</b> <b class=\"match term1\">de</b> Paulo <b class=\"match term1\">de</b> Almeida Nogueira e <b class=\"match term1\">de</b> Ester\n",
       "Nogueira. Seu avô...<b class=\"match term0\">Filho</b> publicou artigo no Diário <b class=\"match term1\">de</b>\n",
       "São Paulo <b class=\"match term1\">de</b> 19 <b class=\"match term1\">de</b> abril <b class=\"match term1\">de</b> 1928 intitulado “O pensamento...<b class=\"match term0\">Filho</b> morreu em São Paulo no dia 29 <b class=\"match term1\">de</b> outubro <b class=\"match term1\">de</b> 1969.\n",
       "\n",
       "Além <b class=\"match term1\">de</b> discursos e conferências"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMÕES FILHO, Ernesto\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "BA) no dia\n",
       "4 <b class=\"match term1\">de</b> outubro <b class=\"match term1\">de</b> 1886, <b class=\"match term2\">filho</b> <b class=\"match term1\">de</b> Ernesto Simões da Silva...decisiva a campanha <b class=\"match term1\">de</b>\n",
       "Simões <b class=\"match term0\">Filho</b> através <b class=\"match term1\">de</b> A Tarde contra seu velho...poder”.\n",
       "\n",
       "No dia 24 <b class=\"match term1\">de</b> novembro <b class=\"match term1\">de</b> 1957, Simões <b class=\"match term0\">Filho</b> veio a falecer, deixando"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMÕES FILHO, Ernesto\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "BA) no dia\n",
       "4 <b class=\"match term1\">de</b> outubro <b class=\"match term1\">de</b> 1886, <b class=\"match term2\">filho</b> <b class=\"match term1\">de</b> Ernesto Simões da Silva...decisiva a campanha <b class=\"match term1\">de</b>\n",
       "Simões <b class=\"match term0\">Filho</b> através <b class=\"match term1\">de</b> A Tarde contra seu velho...poder”.\n",
       "\n",
       "No dia 24 <b class=\"match term1\">de</b> novembro <b class=\"match term1\">de</b> 1957, Simões <b class=\"match term0\">Filho</b> veio a falecer, deixando"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMÕES FILHO, Ernesto\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "BA) no dia\n",
       "4 <b class=\"match term1\">de</b> outubro <b class=\"match term1\">de</b> 1886, <b class=\"match term2\">filho</b> <b class=\"match term1\">de</b> Ernesto Simões da Silva...PRD.\n",
       "\n",
       "## A fundação <b class=\"match term1\">de</b> A Tarde\n",
       "\n",
       "Em outubro <b class=\"match term1\">de</b> 1912 Simões <b class=\"match term0\">Filho</b> fundou o jornal A Tarde...instaurado\n",
       "no dia 10 <b class=\"match term1\">de</b> novembro <b class=\"match term1\">de</b> 1937, Simões <b class=\"match term0\">Filho</b> apoiou em seu estado"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SIMÕES FILHO, Ernesto\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "BA) no dia\n",
       "4 <b class=\"match term1\">de</b> outubro <b class=\"match term1\">de</b> 1886, <b class=\"match term2\">filho</b> <b class=\"match term1\">de</b> Ernesto Simões da Silva...PRD.\n",
       "\n",
       "## A fundação <b class=\"match term1\">de</b> A Tarde\n",
       "\n",
       "Em outubro <b class=\"match term1\">de</b> 1912 Simões <b class=\"match term0\">Filho</b> fundou o jornal A Tarde...instaurado\n",
       "no dia 10 <b class=\"match term1\">de</b> novembro <b class=\"match term1\">de</b> 1937, Simões <b class=\"match term0\">Filho</b> apoiou em seu estado"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MARCONDES FILHO\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Marcondes Machado <b class=\"match term0\">Filho</b>» nasceu na cidade <b class=\"match term1\">de</b> São Paulo em 3\n",
       "<b class=\"match term1\">de</b> agosto <b class=\"match term1\">de</b> 1892, <b class=\"match term2\">filho</b> <b class=\"match term1\">de</b> Alexandre Marcondes...militares.\n",
       "\n",
       "Em 17 <b class=\"match term1\">de</b> julho <b class=\"match term1\">de</b> 1942, Marcondes <b class=\"match term0\">Filho</b> assumiu interinamente...básico do governo <b class=\"match term1\">de</b> Café <b class=\"match term0\">Filho</b>,\n",
       "publicado em 30 <b class=\"match term1\">de</b> março <b class=\"match term1\">de</b> 1955 com o objetivo <b class=\"match term1\">de</b> conciliar todas as\n",
       "principais"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MARCONDES FILHO\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Marcondes Machado <b class=\"match term0\">Filho</b>» nasceu na cidade <b class=\"match term1\">de</b> São Paulo em 3\n",
       "<b class=\"match term1\">de</b> agosto <b class=\"match term1\">de</b> 1892, <b class=\"match term2\">filho</b> <b class=\"match term1\">de</b> Alexandre Marcondes...militares.\n",
       "\n",
       "Em 17 <b class=\"match term1\">de</b> julho <b class=\"match term1\">de</b> 1942, Marcondes <b class=\"match term0\">Filho</b> assumiu interinamente...básico do governo <b class=\"match term1\">de</b> Café <b class=\"match term0\">Filho</b>,\n",
       "publicado em 30 <b class=\"match term1\">de</b> março <b class=\"match term1\">de</b> 1955 com o objetivo <b class=\"match term1\">de</b> conciliar todas as\n",
       "principais"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resultados = busca('\"filho de\"')\n",
    "# print(resultados,type(resultados[0]), len(resultados[0]))\n",
    "# type(resultados)\n",
    "print('Resultados:', len(resultados))\n",
    "for res in resultados[:10]:\n",
    "    print(res[0]['title'])\n",
    "    display(HTML(res[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798fa4f-6a8a-4b47-8c24-e24799d0b7ed",
   "metadata": {},
   "source": [
    "Agora já temos os ingredientes necessários para treinar um modelo de entidades usando a biblioteca spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6ee05fc-74e5-4edd-9151-f48cfd86b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89bca758-a779-41c3-840d-4d02d98dff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTIDO POPULAR DO DISTRITO FEDERAL\n"
     ]
    }
   ],
   "source": [
    "for verb in biograficos.itertuples():\n",
    "    print(verb.title)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f2d72-e48c-49d7-93ec-a2602027654b",
   "metadata": {},
   "source": [
    "Primeiro precisamos criar o conjunto de treinamento do modelo. e deve ter  a forma de uma lista como a descrita abaixo.\n",
    "```python\n",
    "    TRAIN_DATA = [\n",
    "        (\"nasceu em Itaporanga ( PB ) no dia 29 de dezembro de 1944 , filho de Argemiro Abílio de Sousa\", \n",
    "         {\"entities\": [(10, 20, \"LOC\"), (69, 93, \"PERSON\")]}\n",
    "        ),\n",
    "    ]\n",
    "```\n",
    "\n",
    "Como primeira abordagem de treinamento, vamos atualizar o modelo que vem com o spacy adicionando mais um tipo de entidade: \"eventos\". Posteriormente, o modelo pode continuar a ser incrementado adicionando-se outros tipos de entidade. Vamos também aproveitar as entidades reconhecidas pelo modelo base no corpus do DHBB para reforçar o treinamento do modelo no contexto linguístico do DHBB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a047ee40-d024-4a11-b69e-bf26553b7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "path = \"dhbb/dic/evento.txt\"\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    dicio = f.readlines()\n",
    "\n",
    "def gera_dados_treinamento(dicionário, tag):\n",
    "    db = DocBin() # Armazena os exemplos para treinamento em um objeto DocBin\n",
    "    for verb in tqdm(biograficos.itertuples()):\n",
    "        texto = verb.corpo\n",
    "        doc = nlp(texto)\n",
    "        entidades = []#(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        for evento in dicionário:\n",
    "            try:\n",
    "                posini = texto.index(evento)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            posfim = posini + len(evento)\n",
    "            span = doc.char_span(posini,posfim,tag)\n",
    "            if span is not None:\n",
    "                entidades.append(span)\n",
    "        doc.ents = entidades\n",
    "        db.add(doc)\n",
    "    db.to_disk('train.spacy')\n",
    "    return db\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a6a16b7-46a0-4a97-beb3-27ecae21dd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1845it [03:12,  9.60it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 114 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dados \u001b[38;5;241m=\u001b[39m \u001b[43mgera_dados_treinamento\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdicio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEVT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dados\n",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36mgera_dados_treinamento\u001b[0;34m(dicionário, tag)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m span \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m             entidades\u001b[38;5;241m.\u001b[39mappend(span)\n\u001b[0;32m---> 22\u001b[0m     doc\u001b[38;5;241m.\u001b[39ments \u001b[38;5;241m=\u001b[39m entidades\n\u001b[1;32m     23\u001b[0m     db\u001b[38;5;241m.\u001b[39madd(doc)\n\u001b[1;32m     24\u001b[0m db\u001b[38;5;241m.\u001b[39mto_disk(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.spacy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx:762\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx:799\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 114 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "dados = gera_dados_treinamento(dicio, \"EVT\")\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52ef49ab-88a9-4b68-a591-333482a9e0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "an integer is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     dados \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner_training.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     dados \u001b[38;5;241m=\u001b[39m \u001b[43mgera_dados_treinamento\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdicio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEVT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mgera_dados_treinamento\u001b[0;34m(dicionário, tag)\u001b[0m\n\u001b[1;32m     18\u001b[0m         posfim \u001b[38;5;241m=\u001b[39m posini \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(evento)\n\u001b[1;32m     19\u001b[0m         entidades\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mchar_span(posini,posfim,tag))\n\u001b[0;32m---> 20\u001b[0m     doc\u001b[38;5;241m.\u001b[39ments \u001b[38;5;241m=\u001b[39m entidades\n\u001b[1;32m     21\u001b[0m     db\u001b[38;5;241m.\u001b[39madd(doc)\n\u001b[1;32m     22\u001b[0m db\u001b[38;5;241m.\u001b[39mto_disk(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.spacy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx:757\u001b[0m, in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required"
     ]
    }
   ],
   "source": [
    "if os.path.exists('train.spacy'):\n",
    "    print(\"Carregando dados de treinamento do disco...\")\n",
    "    dados = DocBin().from_disk('train.spacy')\n",
    "    dados = pickle.load(open('ner_training.pickle','rb'))\n",
    "else:\n",
    "    dados = gera_dados_treinamento(dicio, \"EVT\")\n",
    "    # pickle.dump(dados,open('ner_training.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1639c2f-d096-40a0-8fae-27a187ca8d05",
   "metadata": {},
   "source": [
    "Agora temos o tipo `EVT`, evento dentre as categorias reconhecidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "339eb851-69b9-4132-95d8-27d9528cfdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EVT', 'LOC', 'MISC', 'ORG', 'PER'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([e[2] for d in dados for e in d[1]['entities']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9f0de-522c-4cc8-9ef5-0e4ade2314ea",
   "metadata": {},
   "source": [
    "Agora reservamos 1000 documentos para testar o reconhecimento de nossa nova categoria, usando o restante para treinar o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab01c12a-d035-4f82-9d67-60a804292bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA = dados[-1000:]\n",
    "TRAIN_DATA = dados[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "339b3b83-278b-4c1a-8d96-cc622a4e1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model=\"pt_core_news_sm\", output_dir=\"dhbb_nlp\", n_iter=25):\n",
    "    \"\"\"Carrega o modelo, configura a pipeline e treina o NER.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  \n",
    "        print(f\"Modelo {model} carregado\")\n",
    "    else:\n",
    "        nlp = spacy.blank(\"pt\")  # cria um novo modelo, vazio.\n",
    "        print(\"Modelo vazio criado para 'pt'\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        else:\n",
    "            nlp.resume_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e68e4474-dbd4-4ebd-93e7-bb43d2826dda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo pt_core_news_sm carregado\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E989] `nlp.update()` was called with two positional arguments. This may be due to a backwards-incompatible change to the format of the training data in spaCy 3.0 onwards. The 'update' function should now be called with a batch of Example objects, instead of `(text, annotation)` tuples. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model, output_dir, n_iter)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[1;32m     39\u001b[0m             texts, annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 40\u001b[0m             \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch of texts\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m \u001b[43m                \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch of annotations\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# dropout - make it harder to memorise data\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLosses\u001b[39m\u001b[38;5;124m\"\u001b[39m, losses)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# test the trained model\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py:1148\u001b[0m, in \u001b[0;36mLanguage.update\u001b[0;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;124;03m\"\"\"Update the models in the pipeline.\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03mexamples (Iterable[Example]): A batch of examples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03mDOCS: https://spacy.io/api/language#update\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE989)\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m losses \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     losses \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: [E989] `nlp.update()` was called with two positional arguments. This may be due to a backwards-incompatible change to the format of the training data in spaCy 3.0 onwards. The 'update' function should now be called with a batch of Example objects, instead of `(text, annotation)` tuples. "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
